{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences \n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: x, self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tflearn.fully_connected(net, 300)\n",
    "        t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "        net = tflearn.activation(\n",
    "            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "def train(sess, env, actor, critic, actor_noise):\n",
    "    TAU = 0.001\n",
    "    ACTOR_LEARNING_RATE = 0.0001\n",
    "    CRITIC_LEARNING_RATE = 0.001\n",
    "    BUFFER_SIZE = 1000000 \n",
    "    MINIBATCH_SIZE = 64\n",
    "    MAX_EPISODES = 50000\n",
    "    MAX_EP_STEPS = 1000\n",
    "    GAMMA = 0.99\n",
    "    USE_GYM_MONITOR = True\n",
    "    RENDER_ENV = True\n",
    "    MONITOR_DIR = \"./\"\n",
    "    \n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\"./\", sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(int(2000), int(0))\n",
    "\n",
    "    # Needed to enable BatchNorm. \n",
    "    # This hurts the performance on Pendulum but could be useful\n",
    "    # in other environments.\n",
    "    # tflearn.is_training(True)\n",
    "\n",
    "    for i in range(int(100)):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(int(1000)):\n",
    "\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "            a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(MINIBATCH_SIZE):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(MINIBATCH_SIZE))\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = critic.predict_target(\n",
    "                    s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(int(MINIBATCH_SIZE)):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(MINIBATCH_SIZE), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "\n",
    "                # Update target networks\n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j)\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f}'.format(int(ep_reward), \\\n",
    "                        i, (ep_ave_max_q / float(j))))\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.001\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "BUFFER_SIZE = 1000000 \n",
    "MINIBATCH_SIZE = 64\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EP_STEPS = 10000\n",
    "GAMMA = 0.99\n",
    "USE_GYM_MONITOR = False\n",
    "RENDER_ENV = True\n",
    "MONITOR_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my myself and i\\sweden 2017-2018\\kth\\ai and ma systems\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Summary name Qmax Value is illegal; using Qmax_Value instead.\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "| Reward: -1680 | Episode: 0 | Qmax: -4.2953\n",
      "| Reward: -1411 | Episode: 1 | Qmax: -0.6630\n",
      "| Reward: -1243 | Episode: 2 | Qmax: -0.5776\n",
      "| Reward: -1649 | Episode: 3 | Qmax: -0.6710\n",
      "| Reward: -1315 | Episode: 4 | Qmax: -0.8352\n",
      "| Reward: -1387 | Episode: 5 | Qmax: -0.7799\n",
      "| Reward: -1490 | Episode: 6 | Qmax: -0.6852\n",
      "| Reward: -1497 | Episode: 7 | Qmax: -0.8157\n",
      "| Reward: -1457 | Episode: 8 | Qmax: -0.6499\n",
      "| Reward: -1422 | Episode: 9 | Qmax: -0.9918\n",
      "| Reward: -1344 | Episode: 10 | Qmax: -1.1685\n",
      "| Reward: -1526 | Episode: 11 | Qmax: -1.6312\n",
      "| Reward: -1519 | Episode: 12 | Qmax: -1.9268\n",
      "| Reward: -1474 | Episode: 13 | Qmax: -2.1538\n",
      "| Reward: -1395 | Episode: 14 | Qmax: -2.0438\n",
      "| Reward: -1423 | Episode: 15 | Qmax: -2.3630\n",
      "| Reward: -1229 | Episode: 16 | Qmax: -1.8161\n",
      "| Reward: -1468 | Episode: 17 | Qmax: -1.6218\n",
      "| Reward: -1708 | Episode: 18 | Qmax: -1.8090\n",
      "| Reward: -1538 | Episode: 19 | Qmax: -1.9391\n",
      "| Reward: -1441 | Episode: 20 | Qmax: -1.7343\n",
      "| Reward: -1506 | Episode: 21 | Qmax: -1.7311\n",
      "| Reward: -1508 | Episode: 22 | Qmax: -1.8377\n",
      "| Reward: -1509 | Episode: 23 | Qmax: -2.3321\n",
      "| Reward: -1428 | Episode: 24 | Qmax: -2.6600\n",
      "| Reward: -1575 | Episode: 25 | Qmax: -2.4130\n",
      "| Reward: -1523 | Episode: 26 | Qmax: -4.8355\n",
      "| Reward: -1470 | Episode: 27 | Qmax: -8.6354\n",
      "| Reward: -1465 | Episode: 28 | Qmax: -9.7492\n",
      "| Reward: -1558 | Episode: 29 | Qmax: -16.0843\n",
      "| Reward: -1517 | Episode: 30 | Qmax: -20.1732\n",
      "| Reward: -1521 | Episode: 31 | Qmax: -20.3413\n",
      "| Reward: -1570 | Episode: 32 | Qmax: -14.1788\n",
      "| Reward: -1544 | Episode: 33 | Qmax: -10.6459\n",
      "| Reward: -1477 | Episode: 34 | Qmax: -11.1235\n",
      "| Reward: -1516 | Episode: 35 | Qmax: -13.0646\n",
      "| Reward: -1576 | Episode: 36 | Qmax: -13.2218\n",
      "| Reward: -933 | Episode: 37 | Qmax: -10.5171\n",
      "| Reward: -1136 | Episode: 38 | Qmax: -9.8259\n",
      "| Reward: -1421 | Episode: 39 | Qmax: -10.0986\n",
      "| Reward: -1601 | Episode: 40 | Qmax: -10.5769\n",
      "| Reward: -1635 | Episode: 41 | Qmax: -11.2009\n",
      "| Reward: -1373 | Episode: 42 | Qmax: -10.3481\n",
      "| Reward: -1409 | Episode: 43 | Qmax: -10.2962\n",
      "| Reward: -1383 | Episode: 44 | Qmax: -9.7385\n",
      "| Reward: -815 | Episode: 45 | Qmax: -5.6133\n",
      "| Reward: -1180 | Episode: 46 | Qmax: -4.2386\n",
      "| Reward: -858 | Episode: 47 | Qmax: -4.6624\n",
      "| Reward: -800 | Episode: 48 | Qmax: -3.5123\n",
      "| Reward: -525 | Episode: 49 | Qmax: -2.5385\n",
      "| Reward: -1496 | Episode: 50 | Qmax: 0.0134\n",
      "| Reward: -1426 | Episode: 51 | Qmax: -0.3018\n",
      "| Reward: -1431 | Episode: 52 | Qmax: -0.0644\n",
      "| Reward: -1136 | Episode: 53 | Qmax: -0.1691\n",
      "| Reward: -1371 | Episode: 54 | Qmax: -0.1848\n",
      "| Reward: -1494 | Episode: 55 | Qmax: -0.5524\n",
      "| Reward: -133 | Episode: 56 | Qmax: 0.2311\n",
      "| Reward: -1522 | Episode: 57 | Qmax: 0.3747\n",
      "| Reward: -1410 | Episode: 58 | Qmax: 0.3854\n",
      "| Reward: -1357 | Episode: 59 | Qmax: 0.1629\n",
      "| Reward: -269 | Episode: 60 | Qmax: 0.1952\n",
      "| Reward: -951 | Episode: 61 | Qmax: 0.3053\n",
      "| Reward: -1151 | Episode: 62 | Qmax: 0.3731\n",
      "| Reward: -994 | Episode: 63 | Qmax: 0.4140\n",
      "| Reward: -1014 | Episode: 64 | Qmax: 0.4846\n",
      "| Reward: -138 | Episode: 65 | Qmax: 0.4543\n",
      "| Reward: -141 | Episode: 66 | Qmax: 0.3359\n",
      "| Reward: -1598 | Episode: 67 | Qmax: 0.0730\n",
      "| Reward: -1589 | Episode: 68 | Qmax: 0.1111\n",
      "| Reward: -403 | Episode: 69 | Qmax: 0.2629\n",
      "| Reward: -127 | Episode: 70 | Qmax: 0.8071\n",
      "| Reward: -1235 | Episode: 71 | Qmax: 1.0736\n",
      "| Reward: -1499 | Episode: 72 | Qmax: 1.1686\n",
      "| Reward: -3 | Episode: 73 | Qmax: 1.2880\n",
      "| Reward: -124 | Episode: 74 | Qmax: 1.2365\n",
      "| Reward: -127 | Episode: 75 | Qmax: 1.3534\n",
      "| Reward: -1337 | Episode: 76 | Qmax: 1.4004\n",
      "| Reward: -1103 | Episode: 77 | Qmax: 1.5502\n",
      "| Reward: -1488 | Episode: 78 | Qmax: 1.5300\n",
      "| Reward: -1135 | Episode: 79 | Qmax: 1.6592\n",
      "| Reward: -1304 | Episode: 80 | Qmax: 1.6551\n",
      "| Reward: -4 | Episode: 81 | Qmax: 1.7440\n",
      "| Reward: -1772 | Episode: 82 | Qmax: 1.8092\n",
      "| Reward: -762 | Episode: 83 | Qmax: 1.7772\n",
      "| Reward: -519 | Episode: 84 | Qmax: 1.7874\n",
      "| Reward: -950 | Episode: 85 | Qmax: 1.7906\n",
      "| Reward: -381 | Episode: 86 | Qmax: 1.9986\n",
      "| Reward: -916 | Episode: 87 | Qmax: 2.0835\n",
      "| Reward: -135 | Episode: 88 | Qmax: 2.1960\n",
      "| Reward: -135 | Episode: 89 | Qmax: 2.4596\n",
      "| Reward: -130 | Episode: 90 | Qmax: 2.6172\n",
      "| Reward: -397 | Episode: 91 | Qmax: 2.3951\n",
      "| Reward: -389 | Episode: 92 | Qmax: 2.5093\n",
      "| Reward: -128 | Episode: 93 | Qmax: 2.8799\n",
      "| Reward: 0 | Episode: 94 | Qmax: 2.7199\n",
      "| Reward: -530 | Episode: 95 | Qmax: 2.7788\n",
      "| Reward: -258 | Episode: 96 | Qmax: 2.8700\n",
      "| Reward: -385 | Episode: 97 | Qmax: 2.8547\n",
      "| Reward: -251 | Episode: 98 | Qmax: 3.0229\n",
      "| Reward: -489 | Episode: 99 | Qmax: 3.2059\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = gym.make(\"Pendulum-v0\")\n",
    "    np.random.seed(int(0))\n",
    "    tf.set_random_seed(int(0))\n",
    "    env.seed(int(0))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high\n",
    "    # Ensure action bound is symmetric\n",
    "    assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "    actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                         float(ACTOR_LEARNING_RATE), float(TAU), int(MINIBATCH_SIZE))\n",
    "\n",
    "    critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                           float(CRITIC_LEARNING_RATE), float(TAU),\n",
    "                           float(GAMMA),\n",
    "                           actor.get_num_trainable_vars())\n",
    "\n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "    if USE_GYM_MONITOR:\n",
    "        if not RENDER_ENV:\n",
    "            env = wrappers.Monitor(\n",
    "                env, MONITOR_DIR, video_callable=False, force=True)\n",
    "        else:\n",
    "            env = wrappers.Monitor(env, MONITOR_DIR, force=True)\n",
    "\n",
    "    train(sess, env, actor, critic, actor_noise)\n",
    "\n",
    "    if USE_GYM_MONITOR:\n",
    "        env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
